{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8s473xDrGF1",
        "outputId": "8d6a8a24-4900-4bd1-c85b-13a8ec95791c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BFFhOKWW6ia"
      },
      "outputs": [],
      "source": [
        "!pip install retina-face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N3wVupCqYalE"
      },
      "outputs": [],
      "source": [
        "from retinaface import RetinaFace\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import imutils\n",
        "\n",
        "import shutil\n",
        "import matplotlib\n",
        "\n",
        "from numba import njit, jit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT-dZa1Hit8Q"
      },
      "source": [
        "# Face Extractor\n",
        "Return the original image but with the face cropped and an increase of 15% in the image area to have some context of the surrounding of the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RBPjXHrSjnmO"
      },
      "outputs": [],
      "source": [
        "class FaceExtractor():\n",
        "    def __call__(self, img):\n",
        "        extracted_faces = RetinaFace.detect_faces(img)\n",
        "        if not isinstance(extracted_faces, dict):\n",
        "            return None # No faces were detected\n",
        "        if not \"face_1\" in extracted_faces:\n",
        "            return None\n",
        "\n",
        "        facial_area = extracted_faces[\"face_1\"][\"facial_area\"]\n",
        "        face_extracted_img = self.crop_image(img, facial_area)\n",
        "        return face_extracted_img\n",
        "\n",
        "    def crop_image(self, img, facial_area):\n",
        "        x1 = facial_area[0]\n",
        "        y1 = facial_area[1]\n",
        "        x2 = facial_area[2]\n",
        "        y2 = facial_area[3]\n",
        "\n",
        "        face_width = x2 - x1\n",
        "        face_height = y2 - y1\n",
        "\n",
        "        original_image_height, original_image_width, _ = img.shape\n",
        "\n",
        "        begin_x, end_x = self.increase_area_from_cropping([x1, x2], face_width, original_image_width)\n",
        "        begin_y, end_y = self.increase_area_from_cropping([y1, y2], face_height, original_image_height)\n",
        "\n",
        "        img = img[begin_y:end_y, begin_x:end_x]\n",
        "\n",
        "        return img\n",
        "\n",
        "    def increase_area_from_cropping(self, xy, face_width_height, original_wh, increase_ratio = 0.1):\n",
        "        begin_xy = xy[0] - int(face_width_height * increase_ratio)\n",
        "        begin_xy = begin_xy if begin_xy >= 0 else 0\n",
        "        end_xy = xy[1] + 1 + int(face_width_height * increase_ratio)\n",
        "        end_xy = end_xy if end_xy <= original_wh else original_wh\n",
        "        return begin_xy, end_xy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iCU-YfDjATn"
      },
      "source": [
        "# Frames with faces extractor\n",
        "Given an image, extracts the number of frames desired for the total of videoclips desired, extract the faces from the frames using the FaceExtractor and return an array of images with the extracted faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZW04ArcFdrc9"
      },
      "outputs": [],
      "source": [
        "def image_resize(img, target_size = 299):\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    if height <= target_size and width <= target_size:\n",
        "        return img\n",
        "\n",
        "    aspect_ratio = width / height\n",
        "    # Reshaping based on the dominat dimension\n",
        "    if aspect_ratio > 1:\n",
        "        resized_img = imutils.resize(img, width = target_size)\n",
        "    else:\n",
        "        resized_img = imutils.resize(img, height = target_size)\n",
        "\n",
        "    return resized_img\n",
        "\n",
        "\n",
        "@njit\n",
        "def calculate_central_pasting_position(target_dimension, original_dimension):\n",
        "    begining = (target_dimension - original_dimension) // 2\n",
        "    ending = begining + original_dimension\n",
        "    return begining, ending\n",
        "\n",
        "@njit\n",
        "def black_border_image(img, target_height = 299, target_width = 299):\n",
        "    original_height, original_width, _ = img.shape\n",
        "\n",
        "    new_img = np.empty((target_height, target_width, 3), dtype=np.int32)\n",
        "    new_img[:] = 0\n",
        "\n",
        "    # Calculate the pasting position to set the face in the center of the image\n",
        "    w1, w2   = calculate_central_pasting_position(target_width, original_width)\n",
        "    h1, h2   = calculate_central_pasting_position(target_height, original_height)\n",
        "\n",
        "    #Placing the face in the center of the image surrounded by black pixels\n",
        "    for h_i in range(h1, h2):\n",
        "        for w_i in range(w1, w2):\n",
        "            new_img[h_i][w_i] = img[h_i - h1][w_i - w1]\n",
        "\n",
        "    return new_img\n",
        "\n",
        "\n",
        "# Check if it is possible to extract the desired number of frames from a video\n",
        "def total_frames_to_extract(video_capture, target_number_of_frames):\n",
        "    video_total_frames = int(cv2.VideoCapture.get(video_capture, cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if target_number_of_frames <= video_total_frames:\n",
        "        return target_number_of_frames\n",
        "    else:\n",
        "        return video_total_frames\n",
        "\n",
        "\n",
        "face_extraction = FaceExtractor()\n",
        "def extract_frames_faces(video_source_path, target_number_of_frames):\n",
        "    # Start OpenCv and check if the file exists\n",
        "    video_capture = cv2.VideoCapture(video_source_path)\n",
        "    if not video_capture.isOpened(): return None\n",
        "\n",
        "    videos_frames = []\n",
        "    frame_counter = 0\n",
        "    total_frames = total_frames_to_extract(video_capture, target_number_of_frames)\n",
        "\n",
        "    while frame_counter != total_frames:\n",
        "        ret, frame = video_capture.read()\n",
        "        if not ret: break\n",
        "\n",
        "        # Extracting the face\n",
        "        face = face_extraction(frame)\n",
        "        if face is None: continue\n",
        "\n",
        "        # Reshaping the face to match a 299x299 size limit and adding a black border\n",
        "        h, w, _ = face.shape\n",
        "        face = image_resize(face)\n",
        "        face = black_border_image(face)\n",
        "\n",
        "        # Adding the face to the list of faces that composes a video\n",
        "        videos_frames.append(face)\n",
        "        frame_counter += 1\n",
        "\n",
        "    return videos_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfOJOR1zmkVL"
      },
      "source": [
        "# Generate videos from frames\n",
        "Receives the name of the original file to save the new videos following a pattern. Checks the number os possible subclips according to the number of frames and generates it saving into the destination path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U7_uugUKl_9t"
      },
      "outputs": [],
      "source": [
        "class VideoGenerator():\n",
        "    def __call__(self, video_name, frames, video_fps, target_number_subclips, destination_path, target_dimension = (299, 299)):\n",
        "        # Checking if the total of subclips desired is feasible\n",
        "        total_frames = len(frames)\n",
        "        possible_number_of_subclips = total_frames //  video_fps\n",
        "        if(possible_number_of_subclips < target_number_subclips):\n",
        "            print(f\"{video_name} possuirÃ¡ {possible_number_of_subclips} subclips - Possui somente {total_frames} frames\")\n",
        "            if possible_number_of_subclips == 0: return\n",
        "\n",
        "        total_subclips = possible_number_of_subclips\n",
        "        total_frames = total_subclips * video_fps\n",
        "        frames = frames[:total_frames]\n",
        "\n",
        "        # Separating the frames into the videos\n",
        "        videos = np.array(frames, dtype=np.uint8)\n",
        "        videos = videos.reshape(total_subclips, video_fps, 299, 299, 3)\n",
        "\n",
        "        # Generating the videos\n",
        "        output_path = os.path.join(destination_path, f\"{video_name}_segment_\")\n",
        "        self.generate_video(videos, output_path, video_fps, target_dimension)\n",
        "\n",
        "\n",
        "    def generate_video(self, videos, destination_path, video_fps, target_dimension):\n",
        "        codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "        for index, video in enumerate(videos):\n",
        "            output_path = destination_path + f\"{(index+1)}.mp4\"\n",
        "\n",
        "            video_output = cv2.VideoWriter(output_path, codec, video_fps, target_dimension)\n",
        "            for frame in video:\n",
        "                video_output.write(frame)\n",
        "\n",
        "            video_output.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79jucgvsmo91"
      },
      "source": [
        "# Video Handler\n",
        "Receive a video path, extract its frames and faces and generates new videos with the number of frames and videos segments as desired"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UJbhhtudnKtW"
      },
      "outputs": [],
      "source": [
        "class VideoHandler():\n",
        "    def __init__(self):\n",
        "        self.video_generator = VideoGenerator()\n",
        "\n",
        "    def __call__(self, video_source_path, destination_path, frames_per_video = 24, total_videos_segments = 1):\n",
        "        video_name = self.get_video_name(video_source_path)\n",
        "\n",
        "        total_frames_to_be_extracted = frames_per_video * total_videos_segments\n",
        "\n",
        "        frames = extract_frames_faces(video_source_path, total_frames_to_be_extracted)\n",
        "        if not (frames is None):\n",
        "            self.video_generator(video_name, frames, frames_per_video, total_videos_segments, destination_path)\n",
        "\n",
        "\n",
        "    def get_video_name(self, video_source_path):\n",
        "        original_video_name = os.path.basename(video_source_path)\n",
        "        original_video_name = original_video_name.split(\".\")[0]\n",
        "        return original_video_name\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNXVMSHhjfle"
      },
      "source": [
        "# Pipeline\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCXP0Eo4e7c_",
        "outputId": "d2b213d0-041b-4443-95fe-e44c8a7a967a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted top-level folder: /content/Celeb-df-V2-faces-extracted\n"
          ]
        }
      ],
      "source": [
        "def delete_files_and_folders(folder_path):\n",
        "    # Iterate over all files and folders in the given path\n",
        "    for root, dirs, files in os.walk(folder_path, topdown=False):\n",
        "        # Delete all files\n",
        "        for file_name in files:\n",
        "            file_path = os.path.join(root, file_name)\n",
        "            os.remove(file_path)\n",
        "\n",
        "        # Delete all folders\n",
        "        for dir_name in dirs:\n",
        "            dir_path = os.path.join(root, dir_name)\n",
        "            os.rmdir(dir_path)\n",
        "            print(f\"Deleted folder: {dir_path}\")\n",
        "\n",
        "    # After all files and folders are deleted, remove the top-level folder itself\n",
        "    os.rmdir(folder_path)\n",
        "    print(f\"Deleted top-level folder: {folder_path}\")\n",
        "\n",
        "delete_files_and_folders(\"/content/Celeb-df-V2-faces-extracted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kl_DOmiQUSjM"
      },
      "outputs": [],
      "source": [
        "def read_txt(txt_file_path, base_path):\n",
        "    with open(txt_file_path, 'r') as file:\n",
        "        dataset = file.read().split(\"\\n\")\n",
        "\n",
        "    dataset = [os.path.join(base_path, video_path) for video_path in dataset]\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FWcj0diGeUMy"
      },
      "outputs": [],
      "source": [
        "base_folder = \"Celeb-df-V2-faces-extracted\"\n",
        "os.mkdir(base_folder)\n",
        "vh = VideoHandler()\n",
        "\n",
        "def make_processing(dictionary, start = -1, end = -1, real_fake_selector = ['real', 'fake']):\n",
        "    os.mkdir(dictionary[\"destination\"])\n",
        "\n",
        "    for real_fake in real_fake_selector:\n",
        "        sub_dict = dictionary[real_fake]\n",
        "\n",
        "        os.mkdir(sub_dict[\"destination\"])\n",
        "\n",
        "        dataset = []\n",
        "        dataset = read_txt(sub_dict[\"txt_file\"], '/content/')\n",
        "\n",
        "        if start != -1 and end != -1:\n",
        "            dataset = dataset[start : end]\n",
        "\n",
        "        total_itens_dataset = len(dataset)\n",
        "\n",
        "        for i, video in enumerate(dataset):\n",
        "            vh(\n",
        "                video_source_path     = video,\n",
        "                destination_path      = sub_dict[\"destination\"],\n",
        "                frames_per_video      = sub_dict[\"frames_per_video\"],\n",
        "                total_videos_segments = sub_dict[\"total_videos_segments\"]\n",
        "            )\n",
        "            if (i+1) % 10 == 0:\n",
        "                print(f\"{i+1}/{total_itens_dataset}\", end = \" | \")\n",
        "\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tkgjml-XAsj"
      },
      "source": [
        "# test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYNY8q-ndVTC",
        "outputId": "92e0b2fc-5324-4a32-f0ac-7ff4ff961cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/179 | 20/179 | 30/179 | 40/179 | 50/179 | 60/179 | 70/179 | 80/179 | 90/179 | 100/179 | 110/179 | 120/179 | 130/179 | 140/179 | 150/179 | 160/179 | 170/179 | \n",
            "\n",
            "10/340 | 20/340 | 30/340 | 40/340 | 50/340 | 60/340 | 70/340 | 80/340 | 90/340 | 100/340 | 110/340 | 120/340 | 130/340 | 140/340 | 150/340 | 160/340 | 170/340 | 180/340 | 190/340 | 200/340 | 210/340 | 220/340 | 230/340 | 240/340 | 250/340 | 260/340 | 270/340 | 280/340 | 290/340 | 300/340 | 310/340 | 320/340 | 330/340 | 340/340 | \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_dict = {\n",
        "    \"destination\": base_folder + \"/test\",\n",
        "\n",
        "    \"real\":{\n",
        "        \"txt_file\": \"/content/test_real.txt\",\n",
        "        \"destination\": base_folder + \"/test/real\",\n",
        "        \"frames_per_video\" : 24,\n",
        "        \"total_videos_segments\": 1\n",
        "    },\n",
        "\n",
        "    \"fake\": {\n",
        "        \"txt_file\": \"/content/test_fake.txt\",\n",
        "        \"destination\": base_folder + \"/test/fake\",\n",
        "        \"frames_per_video\" : 24,\n",
        "        \"total_videos_segments\": 1\n",
        "    }\n",
        "}\n",
        "\n",
        "make_processing(test_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCv-w7_Vqvb8"
      },
      "outputs": [],
      "source": [
        "!zip -r test /content/Celeb-df-V2-faces-extracted/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UF-yB0dzrL_J",
        "outputId": "41ed8c8e-00ea-49f9-dbf7-07da33ae8a92"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/test.zip'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shutil.copy(\"/content/test.zip\", \"/content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL2IKjXEguOG"
      },
      "source": [
        "# val dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WISNefymgxVn",
        "outputId": "ff170089-7003-498c-c4a3-0a00341c8962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/164 | 20/164 | 30/164 | 40/164 | 50/164 | 60/164 | 70/164 | 80/164 | 90/164 | 100/164 | 110/164 | 120/164 | 130/164 | 140/164 | 150/164 | 160/164 | \n",
            "\n",
            "10/327 | 20/327 | 30/327 | 40/327 | 50/327 | 60/327 | 70/327 | 80/327 | 90/327 | 100/327 | 110/327 | 120/327 | 130/327 | 140/327 | 150/327 | 160/327 | 170/327 | 180/327 | 190/327 | 200/327 | 210/327 | 220/327 | 230/327 | 240/327 | 250/327 | 260/327 | 270/327 | 280/327 | 290/327 | 300/327 | 310/327 | 320/327 | \n",
            "\n"
          ]
        }
      ],
      "source": [
        "val_dict = {\n",
        "    \"destination\": base_folder + \"/val\",\n",
        "\n",
        "    \"real\":{\n",
        "        \"txt_file\": \"/content/val_real.txt\",\n",
        "        \"destination\": base_folder + \"/val/real\",\n",
        "        \"frames_per_video\" : 24,\n",
        "        \"total_videos_segments\": 2\n",
        "    },\n",
        "\n",
        "    \"fake\": {\n",
        "        \"txt_file\": \"/content/val_fake.txt\",\n",
        "        \"destination\": base_folder + \"/val/fake\",\n",
        "        \"frames_per_video\" : 24,\n",
        "        \"total_videos_segments\": 1\n",
        "    }\n",
        "}\n",
        "\n",
        "make_processing(val_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmobipgArvFC"
      },
      "outputs": [],
      "source": [
        "!zip -r val.zip /content/Celeb-df-V2-faces-extracted/val\n",
        "shutil.copy(\"/content/val.zip\", \"/content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwEbFACagvg5"
      },
      "source": [
        "# train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO2JbvkIg00b",
        "outputId": "932065c8-d70b-44ac-f962-464dd0919b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24-05-05 16:21:23 - Directory /root/.deepface created\n",
            "24-05-05 16:21:23 - Directory /root/.deepface/weights created\n",
            "24-05-05 16:21:23 - retinaface.h5 will be downloaded from the url https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n",
            "To: /root/.deepface/weights/retinaface.h5\n",
            "100%|ââââââââââ| 119M/119M [00:00<00:00, 178MB/s]\n"
          ]
        }
      ],
      "source": [
        "train_dict = {\n",
        "    \"destination\": base_folder + \"/train\",\n",
        "\n",
        "    \"real\":{\n",
        "        \"txt_file\": \"/content/train_real.txt\",\n",
        "        \"destination\": base_folder + \"/train/real\",\n",
        "        \"frames_per_video\" : 24,\n",
        "        \"total_videos_segments\": 9\n",
        "    },\n",
        "\n",
        "    \"fake\": {\n",
        "        \"txt_file\": \"/content/train_fake.txt\",\n",
        "        \"destination\": base_folder + \"/train/fake\",\n",
        "        \"frames_per_video\" : 24,\n",
        "        \"total_videos_segments\": 1\n",
        "    }\n",
        "}\n",
        "\n",
        "make_processing(train_dict, start = 0, end = 137, real_fake_selector = ['real'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKZTm9PR4L0K"
      },
      "outputs": [],
      "source": [
        "!zip -r train.zip /content/Celeb-df-V2-faces-extracted/train\n",
        "shutil.copy(\"/content/train.zip\", \"/content/drive/MyDrive\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XT-dZa1Hit8Q"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
